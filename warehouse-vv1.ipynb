{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e767aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:01:08.380525Z",
     "iopub.status.busy": "2025-05-25T18:01:08.380199Z",
     "iopub.status.idle": "2025-05-25T18:01:11.918877Z",
     "shell.execute_reply": "2025-05-25T18:01:11.917940Z"
    },
    "papermill": {
     "duration": 3.543745,
     "end_time": "2025-05-25T18:01:11.920136",
     "exception": false,
     "start_time": "2025-05-25T18:01:08.376391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting roboflow\r\n",
      "  Downloading roboflow-1.1.64-py3-none-any.whl.metadata (9.7 kB)\r\n",
      "Collecting ultralytics\r\n",
      "  Downloading ultralytics-8.3.144-py3-none-any.whl.metadata (37 kB)\r\n",
      "Downloading roboflow-1.1.64-py3-none-any.whl (85 kB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ultralytics-8.3.144-py3-none-any.whl (1.0 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ultralytics, roboflow\r\n",
      "Successfully installed roboflow-1.1.64 ultralytics-8.3.144\r\n",
      "üì¶ Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Warehouse AI Monitoring System - YOLOv12 Training Pipeline\n",
    "# Author: AI Assistant\n",
    "# Description: Train YOLOv12 model on warehouse datasets for object detection and counting\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Install required packages\n",
    "!pip install --no-deps roboflow ultralytics\n",
    "\n",
    "\n",
    "print(\"üì¶ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94393736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:01:11.927062Z",
     "iopub.status.busy": "2025-05-25T18:01:11.926815Z",
     "iopub.status.idle": "2025-05-25T18:01:15.678437Z",
     "shell.execute_reply": "2025-05-25T18:01:15.677687Z"
    },
    "papermill": {
     "duration": 3.756491,
     "end_time": "2025-05-25T18:01:15.679964",
     "exception": false,
     "start_time": "2025-05-25T18:01:11.923473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow_heif\r\n",
      "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\r\n",
      "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow_heif) (11.1.0)\r\n",
      "Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pillow_heif\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "roboflow 1.1.64 requires filetype, which is not installed.\r\n",
      "roboflow 1.1.64 requires python-dotenv, which is not installed.\r\n",
      "roboflow 1.1.64 requires idna==3.7, but you have idna 3.10 which is incompatible.\r\n",
      "roboflow 1.1.64 requires opencv-python-headless==4.10.0.84, but you have opencv-python-headless 4.11.0.86 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed pillow_heif-0.22.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow_heif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f677caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:01:15.687451Z",
     "iopub.status.busy": "2025-05-25T18:01:15.686894Z",
     "iopub.status.idle": "2025-05-25T18:01:18.823731Z",
     "shell.execute_reply": "2025-05-25T18:01:18.822735Z"
    },
    "papermill": {
     "duration": 3.141997,
     "end_time": "2025-05-25T18:01:18.825155",
     "exception": false,
     "start_time": "2025-05-25T18:01:15.683158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting filetype\r\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\r\n",
      "Installing collected packages: filetype\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "roboflow 1.1.64 requires python-dotenv, which is not installed.\r\n",
      "roboflow 1.1.64 requires idna==3.7, but you have idna 3.10 which is incompatible.\r\n",
      "roboflow 1.1.64 requires opencv-python-headless==4.10.0.84, but you have opencv-python-headless 4.11.0.86 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed filetype-1.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install filetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af32a888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:01:18.832476Z",
     "iopub.status.busy": "2025-05-25T18:01:18.832201Z",
     "iopub.status.idle": "2025-05-25T18:01:22.160467Z",
     "shell.execute_reply": "2025-05-25T18:01:22.159683Z"
    },
    "papermill": {
     "duration": 3.333512,
     "end_time": "2025-05-25T18:01:22.161960",
     "exception": false,
     "start_time": "2025-05-25T18:01:18.828448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\r\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\r\n",
      "Collecting python-dotenv (from dotenv)\r\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\r\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\r\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\r\n",
      "Installing collected packages: python-dotenv, dotenv\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "roboflow 1.1.64 requires idna==3.7, but you have idna 3.10 which is incompatible.\r\n",
      "roboflow 1.1.64 requires opencv-python-headless==4.10.0.84, but you have opencv-python-headless 4.11.0.86 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed dotenv-0.9.9 python-dotenv-1.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134becac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:01:22.169887Z",
     "iopub.status.busy": "2025-05-25T18:01:22.169617Z",
     "iopub.status.idle": "2025-05-25T18:01:26.418071Z",
     "shell.execute_reply": "2025-05-25T18:01:26.417195Z"
    },
    "papermill": {
     "duration": 4.253753,
     "end_time": "2025-05-25T18:01:26.419295",
     "exception": false,
     "start_time": "2025-05-25T18:01:22.165542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "GPU name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc9b28b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:01:26.427769Z",
     "iopub.status.busy": "2025-05-25T18:01:26.427433Z",
     "iopub.status.idle": "2025-05-26T00:47:57.274286Z",
     "shell.execute_reply": "2025-05-26T00:47:57.273127Z"
    },
    "papermill": {
     "duration": 24390.852965,
     "end_time": "2025-05-26T00:47:57.275618",
     "exception": true,
     "start_time": "2025-05-25T18:01:26.422653",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Downloading Dataset 1: Warehouse General...\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Detection_box-1 to yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 194670/194670 [00:02<00:00, 78348.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Detection_box-1 in yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4016/4016 [00:00<00:00, 5726.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "‚úÖ Dataset 1 downloaded to: /kaggle/working/Detection_box-1\n",
      "üîÑ Downloading Dataset 2: Warehouse Object Detection...\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Vikas_Warehouse_Obj_detection-10 to yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879840/879840 [00:10<00:00, 82667.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Vikas_Warehouse_Obj_detection-10 in yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12454/12454 [00:02<00:00, 4752.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset 2 downloaded to: /kaggle/working/Vikas_Warehouse_Obj_detection-10\n",
      "‚úÖ Dataset download completed!\n",
      "üîÑ Combining datasets...\n",
      "‚úÖ Combined dataset created at: /kaggle/working/combined_warehouse_dataset\n",
      "‚úÖ With 7 unified classes: ['box', 'box_broken', 'forklift', 'open_package', 'package', 'pallets', 'person']\n",
      "üìÅ Total image files processed: 8223\n",
      "üöÄ Starting YOLO model training...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt to 'yolov8x.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 131M/131M [00:00<00:00, 229MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.144 üöÄ Python-3.11.11 torch-2.6.0+cu124 CUDA:auto (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=combined_warehouse_dataset/data.yaml, degrees=0.0, deterministic=True, device=auto, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=35, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8x.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo_warehouse_v1_run, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=warehouse_monitoring_runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=warehouse_monitoring_runs/yolo_warehouse_v1_run, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 17.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=7\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8724709  ultralytics.nn.modules.head.Detect           [7, [320, 640, 640]]          \n",
      "Model summary: 209 layers, 68,159,349 parameters, 68,159,333 gradients\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.35M/5.35M [00:00<00:00, 23.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2533.8¬±1197.1 MB/s, size: 152.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/combined_warehouse_dataset/train/labels... 4445 images, 157 backgrounds, 0 corrupt:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 4602/6725 [00:03<00:01, 1152.71it/s]Image size (108576768 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/combined_warehouse_dataset/train/labels... 6502 images, 223 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6725/6725 [00:05<00:00, 1285.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/combined_warehouse_dataset/train/labels.cache\n",
      "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 227, len(boxes) = 208090. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640 at 60.0% CUDA memory utilization.\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:a (Tesla P100-PCIE-16GB) 15.89G total, 0.60G reserved, 0.57G allocated, 14.72G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "    68159349           0         2.743         143.6         273.5        (1, 3, 640, 640)                    list\n",
      "    68159349           0         3.817         115.5         200.5        (2, 3, 640, 640)                    list\n",
      "    68159349           0         5.683         134.7         313.6        (4, 3, 640, 640)                    list\n",
      "    68159349           0         9.009         254.7         509.4        (8, 3, 640, 640)                    list\n",
      "    68159349           0        19.111         407.7          1127       (16, 3, 640, 640)                    list\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 7 for CUDA:a 9.36G/15.89G (59%) ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2267.7¬±1059.0 MB/s, size: 146.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/combined_warehouse_dataset/train/labels.cache... 6502 images, 223 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6725/6725 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 227, len(boxes) = 208090. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 657.5¬±282.0 MB/s, size: 60.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/combined_warehouse_dataset/valid/labels... 1146 images, 49 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1195/1195 [00:01<00:00, 914.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/combined_warehouse_dataset/valid/labels.cache\n",
      "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 40230. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to warehouse_monitoring_runs/yolo_warehouse_v1_run/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0004921875), 103 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mwarehouse_monitoring_runs/yolo_warehouse_v1_run\u001b[0m\n",
      "Starting training for 35 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/35      6.47G      0.985     0.7356     0.9864        372        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [11:12<00:00,  1.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:31<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.697      0.618      0.659      0.507\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/35      6.76G     0.8743      0.575     0.9487        258        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [11:02<00:00,  1.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:31<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.737      0.661      0.706      0.538\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/35       6.8G     0.8294      0.544     0.9364        403        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [11:00<00:00,  1.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.746       0.69       0.75      0.597\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/35       6.6G     0.7796     0.5053     0.9198        168        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.812      0.686      0.764      0.622\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/35      6.58G     0.7362     0.4712     0.9093        365        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [11:00<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:31<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.744      0.706      0.738      0.604\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/35      6.79G     0.7012     0.4465     0.8988        356        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [11:00<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:31<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.843      0.742      0.812      0.666\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/35      6.67G     0.6698     0.4277     0.8946        239        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:31<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.855      0.734      0.795       0.65\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/35      6.79G      0.655     0.4141     0.8869        279        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.804      0.801      0.829      0.688\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/35      6.75G     0.6387     0.4002     0.8829        166        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.876      0.793       0.84        0.7\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/35      6.76G     0.6263     0.3964     0.8837        268        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230       0.86      0.756      0.834      0.702\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/35      6.72G      0.613      0.385     0.8799        241        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.876      0.789      0.846      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/35      6.69G     0.6017      0.374     0.8744        424        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:58<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.882      0.787       0.84      0.714\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/35      6.64G     0.5919     0.3728     0.8786        348        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:58<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.873      0.777      0.854      0.723\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/35      6.59G     0.5781      0.355     0.8682        246        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.844      0.824      0.871      0.746\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/35      6.72G     0.5699     0.3531       0.87        247        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:58<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.887      0.807      0.873      0.742\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/35      6.68G     0.5619     0.3444     0.8667        200        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:58<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.879      0.806      0.864      0.733\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/35      6.88G     0.5517     0.3395     0.8633        286        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.903      0.782      0.875      0.738\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/35      6.63G     0.5392     0.3304     0.8594        266        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.882      0.825      0.875      0.753\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/35      6.57G     0.5429     0.3302     0.8599        485        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:58<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.877      0.831      0.879      0.751\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/35      6.53G     0.5255     0.3197     0.8566        328        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.847      0.867      0.861      0.746\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/35      6.61G     0.5204     0.3179     0.8569         58        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:58<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.883      0.842      0.885      0.762\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/35      6.63G     0.5162     0.3139     0.8558        260        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:58<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.909      0.817      0.883      0.764\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/35      6.59G     0.5044     0.3021     0.8509         77        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:58<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230       0.89      0.834        0.9      0.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/35      6.55G        0.5     0.3009     0.8519        217        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:58<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.872      0.845      0.884      0.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/35       6.8G     0.4976     0.2978     0.8521        226        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:59<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.898      0.837      0.894      0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/35       6.7G     0.4808     0.3029     0.8522         70        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:56<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.875      0.829      0.858      0.739\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/35      6.53G     0.4743     0.2959     0.8502         64        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:56<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.896      0.841      0.875      0.756\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/35      6.58G     0.4688     0.2902     0.8531        175        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:56<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.906       0.82      0.881      0.773\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/35      6.54G     0.4603     0.2829     0.8494        198        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:56<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.845      0.857      0.871      0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/35      6.51G     0.4537     0.2755     0.8489        147        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:56<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.895       0.84      0.882      0.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/35      6.32G     0.4392     0.2653     0.8431         69        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:56<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.891      0.857       0.89      0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/35      6.51G     0.4261     0.2548     0.8407          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:55<00:00,  1.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.893      0.851      0.889       0.78\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/35      6.59G     0.4212     0.2502      0.841         91        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:56<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.905      0.868      0.905      0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/35      6.57G     0.4125     0.2417     0.8343        118        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:56<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.898      0.864      0.896       0.79\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/35      6.54G     0.3999     0.2296     0.8285        126        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 961/961 [10:56<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:30<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.915      0.858      0.904      0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "35 epochs completed in 6.729 hours.\n",
      "Optimizer stripped from warehouse_monitoring_runs/yolo_warehouse_v1_run/weights/last.pt, 136.7MB\n",
      "Optimizer stripped from warehouse_monitoring_runs/yolo_warehouse_v1_run/weights/best.pt, 136.7MB\n",
      "\n",
      "Validating warehouse_monitoring_runs/yolo_warehouse_v1_run/weights/best.pt...\n",
      "Ultralytics 8.3.144 üöÄ Python-3.11.11 torch-2.6.0+cu124 CUDA:auto (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "Model summary (fused): 112 layers, 68,130,309 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:36<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.914      0.858      0.904      0.797\n",
      "                   box        956      27572      0.987      0.814      0.907      0.808\n",
      "            box_broken         54         54      0.889      0.944      0.933      0.879\n",
      "              forklift        134        134      0.977      0.971      0.993      0.968\n",
      "          open_package         44         47        0.9      0.766      0.849      0.735\n",
      "               package         57         79      0.757      0.684      0.716      0.532\n",
      "               pallets        461      12279      0.994      0.886      0.956      0.874\n",
      "                person         33         65      0.897      0.939      0.972      0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in less\n",
      "invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.1ms preprocess, 21.6ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mwarehouse_monitoring_runs/yolo_warehouse_v1_run\u001b[0m\n",
      "‚úÖ Training completed!\n",
      "üíæ Best model saved at: warehouse_monitoring_runs/yolo_warehouse_v1_run/weights/best.pt\n",
      "üìä Evaluating model performance on the validation set...\n",
      "Ultralytics 8.3.144 üöÄ Python-3.11.11 torch-2.6.0+cu124 CUDA:auto (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "Model summary (fused): 112 layers, 68,130,309 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2650.0¬±914.3 MB/s, size: 188.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/combined_warehouse_dataset/valid/labels.cache... 1146 images, 49 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1195/1195 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 40230. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:39<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1195      40230      0.915      0.858      0.904        0.8\n",
      "                   box        956      27572      0.987      0.814      0.907      0.815\n",
      "            box_broken         54         54      0.889      0.944      0.933      0.879\n",
      "              forklift        134        134      0.977      0.971      0.993       0.97\n",
      "          open_package         44         47      0.901      0.766       0.85       0.74\n",
      "               package         57         79      0.757      0.684      0.714      0.531\n",
      "               pallets        461      12279      0.994      0.887      0.957       0.88\n",
      "                person         33         65      0.897      0.941      0.973      0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in less\n",
      "invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.2ms preprocess, 25.0ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mwarehouse_monitoring_runs/yolo_warehouse_v1_run\u001b[0m\n",
      "mAP50-95 (Box): 0.7999\n",
      "mAP50 (Box): 0.9039\n",
      "üö´ Error during model evaluation: 'DetMetrics' object has no attribute 'metrics'. See valid attributes below.\n",
      "\n",
      "    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP).\n",
      "\n",
      "    Attributes:\n",
      "        save_dir (Path): A path to the directory where the output plots will be saved.\n",
      "        plot (bool): A flag that indicates whether to plot precision-recall curves for each class.\n",
      "        names (dict): A dictionary of class names.\n",
      "        box (Metric): An instance of the Metric class for storing detection results.\n",
      "        speed (dict): A dictionary for storing execution times of different parts of the detection process.\n",
      "        task (str): The task type, set to 'detect'.\n",
      "    \n",
      "üîç Testing model inference...\n",
      "\n",
      "0: 640x640 1 open_package, 30.9ms\n",
      "1: 640x640 3 boxs, 30.9ms\n",
      "2: 640x640 1 box, 30.9ms\n",
      "3: 640x640 1 box, 30.9ms\n",
      "4: 640x640 1 box_broken, 30.9ms\n",
      "Speed: 4.0ms preprocess, 30.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "--- Image 1: Detection_box-1_e38d5ea7-74Opeeen_Packages_jpg.rf.b06e8b4e39f90899cc81a7b3eb64e10c.jpg ---\n",
      "Detected 1 objects\n",
      "Object counts: {'open_package': 1}\n",
      "Saved result to inference_results/result_Detection_box-1_e38d5ea7-74Opeeen_Packages_jpg.rf.b06e8b4e39f90899cc81a7b3eb64e10c.jpg\n",
      "--- Image 2: Vikas_Warehouse_Obj_detection-10_box27_jpeg.rf.929a5198a27ed28c8101d046f7d934bc.jpg ---\n",
      "Detected 3 objects\n",
      "Object counts: {'box': 3}\n",
      "Saved result to inference_results/result_Vikas_Warehouse_Obj_detection-10_box27_jpeg.rf.929a5198a27ed28c8101d046f7d934bc.jpg\n",
      "--- Image 3: Vikas_Warehouse_Obj_detection-10_cardboard880_jpg.rf.3c46c79a13132b2ad0d05805a78c06eb.jpg ---\n",
      "Detected 1 objects\n",
      "Object counts: {'box': 1}\n",
      "Saved result to inference_results/result_Vikas_Warehouse_Obj_detection-10_cardboard880_jpg.rf.3c46c79a13132b2ad0d05805a78c06eb.jpg\n",
      "--- Image 4: Vikas_Warehouse_Obj_detection-10_cardboard1547_jpg.rf.5e2b83667e8fb5eaff6ac1b3a893fd5d.jpg ---\n",
      "Detected 1 objects\n",
      "Object counts: {'box': 1}\n",
      "Saved result to inference_results/result_Vikas_Warehouse_Obj_detection-10_cardboard1547_jpg.rf.5e2b83667e8fb5eaff6ac1b3a893fd5d.jpg\n",
      "--- Image 5: Detection_box-1_fa105a6c-162Open_booox_jpg.rf.7869c30bae529b1ebbe6d4cfed637d61.jpg ---\n",
      "Detected 1 objects\n",
      "Object counts: {'box_broken': 1}\n",
      "Saved result to inference_results/result_Detection_box-1_fa105a6c-162Open_booox_jpg.rf.7869c30bae529b1ebbe6d4cfed637d61.jpg\n",
      "‚úÖ Inference results for 5 images saved in /kaggle/working/inference_results!\n",
      "üì§ Exporting model...\n",
      "Ultralytics 8.3.144 üöÄ Python-3.11.11 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
      "üí° ProTip: Export to OpenVINO format for best performance on Intel CPUs. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'warehouse_monitoring_runs/yolo_warehouse_v1_run/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 11, 8400) (130.4 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnxslim>=0.1.53', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Python 3.11.11 environment at: /usr\n",
      "Resolved 22 packages in 308ms\n",
      "Downloading onnxruntime-gpu (270.1MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 4.1s\n",
      "WARNING ‚ö†Ô∏è \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Downloaded onnxruntime-gpu\n",
      "Prepared 4 packages in 3.25s\n",
      "Installed 4 packages in 13ms\n",
      " + coloredlogs==15.0.1\n",
      " + humanfriendly==10.0\n",
      " + onnxruntime-gpu==1.22.0\n",
      " + onnxslim==0.1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 11.5s, saved as 'warehouse_monitoring_runs/yolo_warehouse_v1_run/weights/best.onnx' (260.1 MB)\n",
      "\n",
      "Export complete (15.3s)\n",
      "Results saved to \u001b[1m/kaggle/working/warehouse_monitoring_runs/yolo_warehouse_v1_run/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=warehouse_monitoring_runs/yolo_warehouse_v1_run/weights/best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=warehouse_monitoring_runs/yolo_warehouse_v1_run/weights/best.onnx imgsz=640 data=combined_warehouse_dataset/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "‚úÖ Model exported successfully to ONNX format: warehouse_monitoring_runs/yolo_warehouse_v1_run/weights/best.onnx\n",
      "üìÅ Main model files location (.pt, .onnx): /kaggle/working/warehouse_monitoring_runs/yolo_warehouse_v1_run/weights\n",
      "\n",
      "============================================================\n",
      "üéâ Warehouse AI Monitoring System Pipeline Complete! üéâ\n",
      "============================================================\n",
      "\n",
      "üîß Use the `warehouse_monitor(image_path, model_path)` function for production inference.\n",
      "   Example: warehouse_monitor('path/to/your/image.jpg', model_path='warehouse_monitoring_runs/yolo_warehouse_v1_run/weights/best.pt')\n",
      "\n",
      "üìä Model Performance Summary:\n",
      "  - Classes configured for training: 7 ['box', 'box_broken', 'forklift', 'open_package', 'package', 'pallets', 'person']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DetMetrics' object has no attribute 'epochs'. See valid attributes below.\n\n    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP).\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class.\n        names (dict): A dictionary of class names.\n        box (Metric): An instance of the Metric class for storing detection results.\n        speed (dict): A dictionary for storing execution times of different parts of the detection process.\n        task (str): The task type, set to 'detect'.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/168774518.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - Classes configured for training: {len(classes)} {classes}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_results_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - Training epochs completed: {training_results_obj.epochs if training_results_obj.epochs else training_args.get('epochs', 'N/A')}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Access actual epochs if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - Best model saved at: {best_model_path_global}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexported_onnx_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;34m\"\"\"Provide a custom attribute access error message with helpful information.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DetMetrics' object has no attribute 'epochs'. See valid attributes below.\n\n    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP).\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class.\n        names (dict): A dictionary of class names.\n        box (Metric): An instance of the Metric class for storing detection results.\n        speed (dict): A dictionary for storing execution times of different parts of the detection process.\n        task (str): The task type, set to 'detect'.\n    "
     ]
    }
   ],
   "source": [
    "# Warehouse AI Monitoring System - YOLO Training Pipeline\n",
    "# Author: AI Assistant (Modified by User, Corrected by Gemini)\n",
    "# Description: Train YOLO model on warehouse datasets with unified class handling\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Download and Setup Datasets\n",
    "# =============================================================================\n",
    "\n",
    "from roboflow import Roboflow\n",
    "\n",
    "def download_datasets():\n",
    "    print(\"\\U0001F504 Downloading Dataset 1: Warehouse General...\")\n",
    "    dataset1 = None # Initialize to None\n",
    "    try:\n",
    "        # IMPORTANT: Replace \"YOUR_ROBOFLOW_API_KEY\" with your actual Roboflow API key\n",
    "        # or use environment variables for better security.\n",
    "        rf1 = Roboflow(api_key=\"yZoots1smL4YKB00QFuC\") # Replace with your key or os.environ.get(\"ROBOFLOW_API_KEY\")\n",
    "        project1 = rf1.workspace(\"blueberry-jtbtk\").project(\"detection_box-q7cw4\")\n",
    "        version1 = project1.version(1)\n",
    "        dataset1 = version1.download(\"yolov8\")\n",
    "        print(f\"\\u2705 Dataset 1 downloaded to: {dataset1.location}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\u26A0\\uFE0F Error downloading dataset 1: {e}\")\n",
    "        dataset1 = None\n",
    "\n",
    "    print(\"\\U0001F504 Downloading Dataset 2: Warehouse Object Detection...\")\n",
    "    dataset2 = None # Initialize to None\n",
    "    try:\n",
    "        # IMPORTANT: Replace \"YOUR_ROBOFLOW_API_KEY\" with your actual Roboflow API key\n",
    "        rf2 = Roboflow(api_key=\"yZoots1smL4YKB00QFuC\") # Replace with your key or os.environ.get(\"ROBOFLOW_API_KEY\")\n",
    "        project2 = rf2.workspace(\"veeck\").project(\"vikas_warehouse_obj_detection\")\n",
    "        version2 = project2.version(10)\n",
    "        dataset2 = version2.download(\"yolov8\")\n",
    "        print(f\"\\u2705 Dataset 2 downloaded to: {dataset2.location}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\u26A0\\uFE0F Error downloading dataset 2: {e}\")\n",
    "        dataset2 = None\n",
    "\n",
    "    if dataset1 is None and dataset2 is None:\n",
    "        raise RuntimeError(\"\\u274C Failed to download any datasets!\")\n",
    "\n",
    "    print(\"\\u2705 Dataset download completed!\")\n",
    "    return dataset1, dataset2\n",
    "\n",
    "# Potentially run download if datasets are not already available\n",
    "# dataset1_obj, dataset2_obj = download_datasets()\n",
    "# For testing without re-downloading, you might manually set these paths if already downloaded:\n",
    "# from types import SimpleNamespace\n",
    "# dataset1_obj = SimpleNamespace(location='./Warehouse-General-1') # Example path\n",
    "# dataset2_obj = SimpleNamespace(location='./Vikas_Warehouse_Obj_Detection-10') # Example path\n",
    "# Ensure these paths exist and contain the data.yaml and image/label folders.\n",
    "# For now, we'll call the download function.\n",
    "dataset1_obj, dataset2_obj = download_datasets()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Combine Datasets\n",
    "# =============================================================================\n",
    "\n",
    "def combine_datasets(ds1_obj, ds2_obj):\n",
    "    print(\"\\U0001F504 Combining datasets...\")\n",
    "\n",
    "    combined_path = Path(\"./combined_warehouse_dataset\")\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        (combined_path / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (combined_path / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dataset_locations = []\n",
    "    if ds1_obj and hasattr(ds1_obj, 'location') and Path(ds1_obj.location).exists():\n",
    "        dataset_locations.append(Path(ds1_obj.location))\n",
    "    else:\n",
    "        print(\"Dataset 1 location is invalid or not found. Skipping.\")\n",
    "\n",
    "    if ds2_obj and hasattr(ds2_obj, 'location') and Path(ds2_obj.location).exists():\n",
    "        dataset_locations.append(Path(ds2_obj.location))\n",
    "    else:\n",
    "        print(\"Dataset 2 location is invalid or not found. Skipping.\")\n",
    "\n",
    "    if not dataset_locations:\n",
    "        print(\"\\U0001F6AB No valid datasets to combine.\")\n",
    "        return None, []\n",
    "\n",
    "    all_classes = set()\n",
    "    processed_dataset_info = []\n",
    "\n",
    "    for loc_path in dataset_locations:\n",
    "        yaml_path = loc_path / \"data.yaml\"\n",
    "        if yaml_path.exists():\n",
    "            with open(yaml_path, 'r') as f:\n",
    "                data = yaml.safe_load(f)\n",
    "                names = data.get('names', [])\n",
    "                if isinstance(names, list) and all(isinstance(item, str) for item in names): # Standard list of names\n",
    "                    pass\n",
    "                elif isinstance(names, dict): # Dictionary like {0: 'name1', 1: 'name2'}\n",
    "                    names = list(names.values())\n",
    "                else:\n",
    "                    print(f\"\\u26A0\\uFE0F Warning: Class names in {yaml_path} are not in expected list or dict format. Skipping this dataset's classes for unification.\")\n",
    "                    continue # Skip if names format is unexpected for class processing\n",
    "                \n",
    "                names = [str(n).lower() for n in names]  # Normalize class names to lowercase strings\n",
    "                processed_dataset_info.append({'path': loc_path, 'class_map': names})\n",
    "                all_classes.update(names)\n",
    "        else:\n",
    "            print(f\"\\u26A0\\uFE0F Warning: data.yaml not found in {loc_path}. Skipping this dataset for combination.\")\n",
    "\n",
    "\n",
    "    if not all_classes:\n",
    "        print(\"\\U0001F6AB No classes found in any dataset. Cannot combine.\")\n",
    "        return None, []\n",
    "\n",
    "    unified_classes = sorted(list(all_classes))\n",
    "\n",
    "    def remap_label(label_path, class_map_old_dataset, current_unified_classes):\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            try:\n",
    "                old_cls_idx = int(parts[0])\n",
    "                if 0 <= old_cls_idx < len(class_map_old_dataset):\n",
    "                    class_name_original = class_map_old_dataset[old_cls_idx] # Already lowercased during collection\n",
    "                    if class_name_original in current_unified_classes:\n",
    "                        new_cls_idx = current_unified_classes.index(class_name_original)\n",
    "                        new_line = ' '.join([str(new_cls_idx)] + parts[1:])\n",
    "                        new_lines.append(new_line)\n",
    "                    else:\n",
    "                        print(f\"\\U0001F6AB Warning: Class '{class_name_original}' from {label_path} not in unified_classes. Skipping line.\")\n",
    "                else:\n",
    "                    print(f\"\\U0001F6AB Warning: Invalid class index {old_cls_idx} in {label_path}. Skipping line.\")\n",
    "            except ValueError:\n",
    "                print(f\"\\U0001F6AB Warning: Malformed line in {label_path}: {line.strip()}. Skipping line.\")\n",
    "            except IndexError:\n",
    "                 print(f\"\\U0001F6AB Warning: Index error for class mapping in {label_path}. Old class map length: {len(class_map_old_dataset)}, index: {old_cls_idx}. Skipping line.\")\n",
    "        return new_lines\n",
    "\n",
    "    file_count = 0\n",
    "    for i, ds_info in enumerate(processed_dataset_info):\n",
    "        dataset_path = ds_info['path']\n",
    "        class_map_old = ds_info['class_map']\n",
    "        # Use a unique prefix, e.g., based on dataset folder name if possible, or stick to ds{i+1}\n",
    "        prefix = f\"{dataset_path.name.replace(' ', '_')}_\" # Make prefix from dataset folder name\n",
    "\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            src_img_dir = dataset_path / split / 'images'\n",
    "            src_lbl_dir = dataset_path / split / 'labels'\n",
    "            \n",
    "            if not src_img_dir.exists():\n",
    "                # print(f\"Info: Image directory not found for {split} in {dataset_path}, skipping.\")\n",
    "                continue\n",
    "            if not src_lbl_dir.exists():\n",
    "                # print(f\"Info: Label directory not found for {split} in {dataset_path}, skipping image copying for this split as labels are crucial.\")\n",
    "                # continue # If labels must exist, uncomment this\n",
    "                pass # If images without labels are okay, comment above and keep pass\n",
    "\n",
    "            for img_file in src_img_dir.glob('*'):\n",
    "                if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.webp']:\n",
    "                    dst_img_name = f\"{prefix}{img_file.name}\"\n",
    "                    dst_img = combined_path / split / 'images' / dst_img_name\n",
    "                    shutil.copy2(img_file, dst_img)\n",
    "                    file_count += 1\n",
    "\n",
    "                    lbl_file_name = f\"{img_file.stem}.txt\"\n",
    "                    lbl_file = src_lbl_dir / lbl_file_name\n",
    "                    if lbl_file.exists():\n",
    "                        remapped_lines = remap_label(lbl_file, class_map_old, unified_classes)\n",
    "                        if remapped_lines: # Only write if there's content\n",
    "                            dst_lbl_name = f\"{prefix}{img_file.stem}.txt\"\n",
    "                            dst_lbl = combined_path / split / 'labels' / dst_lbl_name\n",
    "                            with open(dst_lbl, 'w') as f:\n",
    "                                f.write('\\n'.join(remapped_lines))\n",
    "                        # else: # Optional: print if a label file becomes empty after remapping\n",
    "                        #     print(f\"Info: Label file {lbl_file} resulted in no valid annotations after remapping.\")\n",
    "                    # else: # Optional: print if an image doesn't have a corresponding label file\n",
    "                    #    print(f\"Info: Label file not found for image {img_file}, copied image only.\")\n",
    "\n",
    "\n",
    "    combined_yaml_path = combined_path / 'data.yaml'\n",
    "    combined_yaml_content = {\n",
    "        'path': str(combined_path.resolve()), # Root path of the dataset\n",
    "        'train': str(Path('train') / 'images'), # Relative to 'path'\n",
    "        'val': str(Path('valid') / 'images'),   # Relative to 'path'\n",
    "        'test': str(Path('test') / 'images'),  # Relative to 'path'\n",
    "        'nc': len(unified_classes),\n",
    "        'names': unified_classes\n",
    "    }\n",
    "\n",
    "    with open(combined_yaml_path, 'w') as f:\n",
    "        yaml.dump(combined_yaml_content, f, default_flow_style=None, sort_keys=False)\n",
    "\n",
    "    print(f\"\\u2705 Combined dataset created at: {combined_path.resolve()}\")\n",
    "    print(f\"\\u2705 With {len(unified_classes)} unified classes: {unified_classes}\")\n",
    "    print(f\"\\U0001F4C1 Total image files processed: {file_count}\")\n",
    "\n",
    "    return combined_path, unified_classes\n",
    "\n",
    "combined_dataset_path, classes = combine_datasets(dataset1_obj, dataset2_obj)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: Train YOLO Model\n",
    "# =============================================================================\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def train_warehouse_model(dataset_yaml_path, class_list):\n",
    "    if not dataset_yaml_path or not Path(dataset_yaml_path).exists():\n",
    "        print(\"\\U0001F6AB Training cannot start: Combined dataset YAML not found.\")\n",
    "        return None, None\n",
    "    if not class_list:\n",
    "        print(\"\\U0001F6AB Training cannot start: No classes defined for the model.\")\n",
    "        return None, None\n",
    "\n",
    "    print(\"\\U0001F680 Starting YOLO model training...\")\n",
    "    # Use a standard Ultralytics model, e.g., yolov8x.pt (large), yolov8n.pt (small)\n",
    "    # If 'yolo11x.pt' is your custom pre-trained model or specific architecture, ensure it's correctly referenced.\n",
    "    # For this example, we use 'yolov8x.pt'. Ultralytics will download it if not present.\n",
    "    model = YOLO('yolov8x.pt') \n",
    "\n",
    "    training_args = {\n",
    "        'data': str(dataset_yaml_path),\n",
    "        'epochs': 35,       # Consider reducing for initial testing (e.g., 3-10 epochs)\n",
    "        'imgsz': 640,\n",
    "        'batch': -1,         # Adjust based on your GPU memory (e.g., 8, 4, or -1 for auto-batch)\n",
    "        'device': 'auto',    # Use '0' for GPU 0, 'cpu', or 'auto'\n",
    "        'patience': 15,      # Consider increasing if epochs are high\n",
    "        'save': True,\n",
    "        'cache': False,       # Set to True or 'ram'/'disk' if I/O is slow and you have resources\n",
    "        'workers': 4,        # Adjust based on your CPU cores and dataloader performance\n",
    "        'project': 'warehouse_monitoring_runs', # Changed project name slightly for clarity\n",
    "        'name': 'yolo_warehouse_v1_run',       # Changed experiment name slightly\n",
    "        'exist_ok': True,    # Allows re-running and overwriting previous experiment with same name\n",
    "        'verbose': True,\n",
    "        # 'logger': True, # Removed: Not a standard argument, logging is handled by Ultralytics\n",
    "    }\n",
    "    \n",
    "    # Check if number of classes in the model matches the dataset\n",
    "    # This is typically handled by YOLO if starting from a pretrained model with different nc,\n",
    "    # but good to be aware of. The model's head will be reinitialized.\n",
    "    \n",
    "    results = model.train(**training_args)\n",
    "    print(\"\\u2705 Training completed!\")\n",
    "    \n",
    "    # Save the path to the best model for later use\n",
    "    # The actual path is inside results.save_dir or model.trainer.save_dir\n",
    "    best_model_path = Path(results.save_dir) / 'weights' / 'best.pt'\n",
    "    print(f\"\\U0001F4BE Best model saved at: {best_model_path}\")\n",
    "    \n",
    "    return model, results, best_model_path\n",
    "\n",
    "\n",
    "# Ensure combined_dataset_path and classes are valid before training\n",
    "trained_model = None\n",
    "training_results_obj = None\n",
    "best_model_path_global = None\n",
    "\n",
    "if combined_dataset_path and classes:\n",
    "    data_yaml_for_training = combined_dataset_path / 'data.yaml'\n",
    "    trained_model, training_results_obj, best_model_path_global = train_warehouse_model(data_yaml_for_training, classes)\n",
    "else:\n",
    "    print(\"\\U0001F6AB Skipping training due to issues in dataset combination.\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: Evaluate Model Performance\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(model_to_evaluate, data_yaml_path_for_eval):\n",
    "    if model_to_evaluate is None:\n",
    "        print(\"\\U0001F6AB No model trained. Skipping evaluation.\")\n",
    "        return None\n",
    "    if not data_yaml_path_for_eval or not Path(data_yaml_path_for_eval).exists():\n",
    "        print(\"\\U0001F6AB Cannot evaluate: Dataset YAML for validation not found.\")\n",
    "        return None\n",
    "        \n",
    "    print(\"\\U0001F4CA Evaluating model performance on the validation set...\")\n",
    "    try:\n",
    "        # Ensure the model uses the correct data configuration for validation\n",
    "        val_results = model_to_evaluate.val(data=str(data_yaml_path_for_eval), split='val')\n",
    "        print(f\"mAP50-95 (Box): {val_results.box.map:.4f}\")\n",
    "        print(f\"mAP50 (Box): {val_results.box.map50:.4f}\")\n",
    "        # Precision and Recall might need to be accessed differently or might not be directly on box\n",
    "        # For specific P, R:\n",
    "        # print(f\"Precision: {val_results.box.mp:.4f}\") # mp might not exist directly like this\n",
    "        # print(f\"Recall: {val_results.box.mr:.4f}\") # mr might not exist directly like this\n",
    "        # Ultralytics typically prints these during validation. The results object contains detailed metrics.\n",
    "        # Access general metrics\n",
    "        print(f\"All metrics: {val_results.metrics}\") # Contains precision, recall, mAP etc.\n",
    "        return val_results\n",
    "    except Exception as e:\n",
    "        print(f\"\\U0001F6AB Error during model evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if trained_model and combined_dataset_path:\n",
    "    data_yaml_for_eval = combined_dataset_path / 'data.yaml'\n",
    "    validation_results = evaluate_model(trained_model, data_yaml_for_eval)\n",
    "else:\n",
    "    print(\"\\U0001F6AB Skipping model evaluation.\")\n",
    "    validation_results = None\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: Test Model on Sample Images\n",
    "# =============================================================================\n",
    "\n",
    "def test_model_inference(model_to_test, dataset_base_path, num_images=5):\n",
    "    if model_to_test is None:\n",
    "        print(\"\\U0001F6AB No model available. Skipping inference test.\")\n",
    "        return\n",
    "    if not dataset_base_path or not Path(dataset_base_path).exists():\n",
    "        print(\"\\U0001F6AB Dataset path for testing not found. Skipping inference test.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\U0001F50D Testing model inference...\")\n",
    "    test_img_dir = Path(dataset_base_path) / 'test' / 'images'\n",
    "    \n",
    "    if not test_img_dir.exists():\n",
    "        print(f\"\\U0001F6AB Test images directory not found at: {test_img_dir}\")\n",
    "        return\n",
    "\n",
    "    test_images = list(test_img_dir.glob('*.jpg')) + list(test_img_dir.glob('*.jpeg')) + list(test_img_dir.glob('*.png'))\n",
    "    \n",
    "    if not test_images:\n",
    "        print(f\"\\U0001F6AB No test images found in {test_img_dir} with .jpg, .jpeg, or .png extensions.\")\n",
    "        return\n",
    "\n",
    "    sample_test_images = test_images[:num_images]\n",
    "    if not sample_test_images:\n",
    "        print(\"\\U0001F6AB Not enough test images to sample from.\") # Should be caught by previous check too\n",
    "        return\n",
    "\n",
    "    results_list = model_to_test(sample_test_images) # Perform inference\n",
    "    \n",
    "    inference_output_dir = Path(\"./inference_results\")\n",
    "    inference_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, result in enumerate(results_list):\n",
    "        print(f\"--- Image {i+1}: {Path(sample_test_images[i]).name} ---\")\n",
    "        print(f\"Detected {len(result.boxes)} objects\")\n",
    "        class_counts = {}\n",
    "        if hasattr(model_to_test, 'names') and model_to_test.names:\n",
    "            model_class_names = model_to_test.names\n",
    "        else: # Fallback if model.names isn't populated as expected\n",
    "            model_class_names = {k: f\"class_{k}\" for k in range(result.boxes.cls.max().item() + 1)} if len(result.boxes) > 0 else {}\n",
    "\n",
    "\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls)\n",
    "            class_name = model_class_names.get(class_id, f\"unknown_class_{class_id}\")\n",
    "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "        print(f\"Object counts: {class_counts}\")\n",
    "        \n",
    "        # Save the image with detections\n",
    "        save_path = inference_output_dir / f\"result_{Path(sample_test_images[i]).stem}.jpg\"\n",
    "        result.save(filename=str(save_path))\n",
    "        print(f\"Saved result to {save_path}\")\n",
    "        \n",
    "    print(f\"\\u2705 Inference results for {len(sample_test_images)} images saved in {inference_output_dir.resolve()}!\")\n",
    "\n",
    "if trained_model and combined_dataset_path:\n",
    "    test_model_inference(trained_model, combined_dataset_path)\n",
    "else:\n",
    "    print(\"\\U0001F6AB Skipping model inference test.\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: Export Model for Production\n",
    "# =============================================================================\n",
    "\n",
    "def export_trained_model(model_to_export, export_path_base):\n",
    "    if model_to_export is None:\n",
    "        print(\"\\U0001F6AB No model trained. Skipping export.\")\n",
    "        return None\n",
    "    if export_path_base is None or not Path(export_path_base).parent.exists(): # Check if parent of best.pt exists\n",
    "        print(f\"\\U0001F6AB Export path base seems invalid ({export_path_base}). Cannot determine export directory. Skipping export.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\U0001F4E4 Exporting model...\")\n",
    "    try:\n",
    "        # Exporting to ONNX format. Other formats: 'torchscript', 'coreml', 'engine', 'pb', etc.\n",
    "        # The export path will be relative to the `save_dir` of the model, or can be specified.\n",
    "        # If `model_to_export` is the model object from `train`, its `export` method handles paths well.\n",
    "        # Let's ensure the export directory is based on the actual run.\n",
    "        \n",
    "        onnx_file_path = model_to_export.export(format='onnx', imgsz=640) # imgsz can be specified for export\n",
    "        print(f\"\\u2705 Model exported successfully to ONNX format: {onnx_file_path}\")\n",
    "        \n",
    "        # The .pt file is already saved (best.pt and last.pt) in the training run directory.\n",
    "        # The `export_path_base` here refers to the `best.pt` path from training.\n",
    "        export_dir = Path(export_path_base).parent # weights directory\n",
    "        print(f\"\\U0001F4C1 Main model files location (.pt, .onnx): {export_dir.resolve()}\")\n",
    "        return onnx_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"\\U0001F6AB Error during model export: {e}\")\n",
    "        return None\n",
    "\n",
    "exported_onnx_path = None\n",
    "if trained_model and best_model_path_global:\n",
    "    exported_onnx_path = export_trained_model(trained_model, best_model_path_global)\n",
    "else:\n",
    "    print(\"\\U0001F6AB Skipping model export.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: Create Inference Function for Production\n",
    "# =============================================================================\n",
    "\n",
    "def warehouse_monitor(image_path, model_path=None, confidence_threshold=0.5):\n",
    "    if model_path is None:\n",
    "        print(\"\\U0001F6AB Error: Model path not provided for warehouse_monitor.\")\n",
    "        # Fallback to a default expected path if best_model_path_global is set\n",
    "        if best_model_path_global and Path(best_model_path_global).exists():\n",
    "            print(f\"Using global best model path: {best_model_path_global}\")\n",
    "            model_path = best_model_path_global\n",
    "        else: # Try to construct the path from training args (less reliable if names changed)\n",
    "            default_path = Path('warehouse_monitoring_runs') / 'yolo_warehouse_v1_run' / 'weights' / 'best.pt'\n",
    "            print(f\"Attempting to use default path: {default_path}\")\n",
    "            if default_path.exists():\n",
    "                model_path = default_path\n",
    "            else:\n",
    "                print(f\"\\U0001F6AB Default model path {default_path} not found. Please specify a valid model_path.\")\n",
    "                return None\n",
    "                \n",
    "    if not Path(model_path).exists():\n",
    "        print(f\"\\U0001F6AB Error: Model file not found at {model_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        inference_model = YOLO(str(model_path)) # Load the specific model\n",
    "    except Exception as e:\n",
    "        print(f\"\\U0001F6AB Error loading model {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\U0001F52D Performing inference with model: {model_path} on image: {image_path}\")\n",
    "    results = inference_model(image_path, conf=confidence_threshold)\n",
    "\n",
    "    detection_summary = {\n",
    "        'image_path': str(image_path),\n",
    "        'total_objects': 0,\n",
    "        'class_counts': {},\n",
    "        'detections': [] # List of {'class': name, 'confidence': conf, 'bbox': [x1,y1,x2,y2]}\n",
    "    }\n",
    "\n",
    "    if hasattr(inference_model, 'names') and inference_model.names:\n",
    "        model_class_names = inference_model.names\n",
    "    else: # Fallback if model.names isn't populated as expected after loading\n",
    "        print(\"\\U0001F6AB Warning: Model class names not found directly on loaded model. Will use 'class_ID' format.\")\n",
    "        model_class_names = {}\n",
    "\n",
    "\n",
    "    for result in results: # Iterates over images, though here we pass one image path\n",
    "        if result.boxes is not None:\n",
    "            detection_summary['total_objects'] = len(result.boxes)\n",
    "            for box in result.boxes:\n",
    "                class_id = int(box.cls)\n",
    "                # Try to get class name, fallback if model_class_names is empty or ID is missing\n",
    "                class_name = model_class_names.get(class_id, f\"class_{class_id}\")\n",
    "                \n",
    "                confidence = float(box.conf)\n",
    "                \n",
    "                detection_summary['class_counts'][class_name] = \\\n",
    "                    detection_summary['class_counts'].get(class_name, 0) + 1\n",
    "                \n",
    "                detection_summary['detections'].append({\n",
    "                    'class': class_name,\n",
    "                    'confidence': round(confidence, 4),\n",
    "                    'bbox': [round(coord, 2) for coord in box.xyxy.tolist()[0]] # x1, y1, x2, y2\n",
    "                })\n",
    "    return detection_summary\n",
    "\n",
    "# =============================================================================\n",
    "# Final Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\U0001F389 Warehouse AI Monitoring System Pipeline Complete! \\U0001F389\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "if trained_model and best_model_path_global:\n",
    "    print(\"\\U0001F527 Use the `warehouse_monitor(image_path, model_path)` function for production inference.\")\n",
    "    print(f\"   Example: warehouse_monitor('path/to/your/image.jpg', model_path='{best_model_path_global}')\\n\")\n",
    "    print(\"\\U0001F4CA Model Performance Summary:\")\n",
    "    if classes:\n",
    "        print(f\"  - Classes configured for training: {len(classes)} {classes}\")\n",
    "    if training_results_obj:\n",
    "        print(f\"  - Training epochs completed: {training_results_obj.epochs if training_results_obj.epochs else training_args.get('epochs', 'N/A')}\") # Access actual epochs if available\n",
    "    print(f\"  - Best model saved at: {best_model_path_global}\")\n",
    "    if exported_onnx_path:\n",
    "        print(f\"  - Model exported to ONNX at: {exported_onnx_path}\")\n",
    "    if validation_results and hasattr(validation_results, 'box'):\n",
    "        print(f\"  - Validation mAP50-95 (Box): {validation_results.box.map:.4f}\")\n",
    "        print(f\"  - Validation mAP50 (Box): {validation_results.box.map50:.4f}\")\n",
    "else:\n",
    "    print(\"\\U0001F6AB Model training or setup was not fully completed. Please check logs.\")\n",
    "    if not combined_dataset_path or not classes:\n",
    "         print(\"  - Issue: Dataset combination failed or yielded no classes.\")\n",
    "    elif not trained_model:\n",
    "         print(\"  - Issue: Model training did not complete successfully.\")\n",
    "\n",
    "\n",
    "# Example of how to use the inference function (if a model was trained):\n",
    "# if best_model_path_global and combined_dataset_path:\n",
    "#     print(\"\\n\" + \"=\"*30 + \" EXAMPLE INFERENCE \" + \"=\"*30)\n",
    "#     test_img_dir_for_example = combined_dataset_path / 'test' / 'images'\n",
    "#     example_images = list(test_img_dir_for_example.glob('*.jpg')) + \\\n",
    "#                      list(test_img_dir_for_example.glob('*.jpeg')) + \\\n",
    "#                      list(test_img_dir_for_example.glob('*.png'))\n",
    "#     if example_images:\n",
    "#         example_image_path = example_images[0]\n",
    "#         print(f\"Running inference on example image: {example_image_path}\")\n",
    "#         summary = warehouse_monitor(image_path=str(example_image_path), model_path=str(best_model_path_global))\n",
    "#         if summary:\n",
    "#             print(\"Inference Summary:\")\n",
    "#             import json\n",
    "#             print(json.dumps(summary, indent=2))\n",
    "#     else:\n",
    "#         print(\"No example images found in the test set to run a sample inference.\")\n",
    "# else:\n",
    "#     print(\"\\nSkipping example inference as model path or dataset path is not available.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24419.337981,
   "end_time": "2025-05-26T00:48:03.486453",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-25T18:01:04.148472",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
